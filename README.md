# Faithful Low-resource Data-to-Text Generation through Cycle Training

Howdy!

Our code release is under Amazon's internal approval process.
The code will be released at https://github.com/amzn, and we will update the repository link once approved. 
In the meantime, you are very welcome to contact us if you need any implementation assistance to replicate the work prior to the official code release.

## Cite us
Please use the following bibtex when referencing this work:
```
@inproceedings{wang-etal-2023-cyclenlg,
  title = {Faithful Low-resource Data-to-Text Generation through Cycle Training},
  author = {Wang, Zhuoer and Collins, Marcus and Vedula, Nikhita and Filice, Simone and Malmasi, Shervin and Rokhlenko, Oleg},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)},
  month = jul,
  year = {2023},
  address = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  abbr = {ACL}
}
```
